{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdb6ffe6",
   "metadata": {},
   "source": [
    "**Интерфейс для удобного общения с виртуальным помощником:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16abcbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import clickhouse_connect\n",
    "from embedding import E5LargeEmbeddingFunction\n",
    "from similarity import bm25_ensemble_with_crossenc_answer, prep_query, bm25_fewshot_with_cross_encoder\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "\n",
    "client = clickhouse_connect.get_client(host='y1jzidyt9q.us-east-2.aws.clickhouse.cloud', port=8443, username='default', password='_lQ_JWXYQD3ym')\n",
    "emb_func = E5LargeEmbeddingFunction()\n",
    "\n",
    "def message_llm(system_prompt, user_input, temperature=0.6):\n",
    "    '''Function to message (test) our LLM'''\n",
    "\n",
    "    try:\n",
    "        url = \"https://live-relaxed-oryx.ngrok-free.app/v1/chat/completions\"\n",
    "\n",
    "        data = {\n",
    "          \"messages\": [\n",
    "            { \"role\": \"system\", \"content\": system_prompt},\n",
    "            { \"role\": \"user\", \"content\": user_input}\n",
    "          ],\n",
    "          \"temperature\": 0.6,\n",
    "          # \"max_tokens\": -1,\n",
    "          # \"stream\": False\n",
    "        }\n",
    "\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return eval(response.text.replace(\"\\n  \", \"\"))['choices'][0]['message']['content']\n",
    "        else:\n",
    "            print(\"Ошибка запроса:\\n\", response.status_code, response.text)\n",
    "            return \"\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        \n",
    "        print(f\"Error {e}!\")\n",
    "        \n",
    "rag_prompt = \"\"\"\n",
    "### CONTEXT ###\n",
    "\n",
    "1) ИСТОЧНИК 1: \n",
    "- {type_of_source1}\n",
    "- {link_to_source1}\n",
    "- {batch_1}\n",
    "\n",
    "2) ИСТОЧНИК 2: \n",
    "- {type_of_source2}\n",
    "- {link_to_source2}\n",
    "- {batch_2}\n",
    "\n",
    "3) ИСТОЧНИК 3:\n",
    "- {type_of_source3}\n",
    "- {link_to_source3}\n",
    "- {batch_3}\n",
    "\n",
    "### USER ###\n",
    "\n",
    "На основании источников 1,2,3 дай исчерпывающий ответ на\n",
    "приведенный ниже вопрос. Если ты неуверен, что можешь дать ответ на основании \n",
    "источников 1,2,3 пиши «Я не могу ответить на этот вопрос.».\n",
    "- {QUESTION}\n",
    "\"\"\"\n",
    "\n",
    "few_shot_data = pd.read_csv('data/finetune_df_final.csv', index_col=0)\n",
    "few_shot_data\n",
    "\n",
    "rag_prompt_fw_ex = \"\"\"\n",
    "### EXAMPLES ###\n",
    "\n",
    "1) Пример 1:\n",
    "КОНТЕКСТ: {context_1}\n",
    "ВОПРОС: {question_1}\n",
    "ОТВЕТ: {answer_1}\n",
    "\n",
    "2) Пример 2:\n",
    "КОНТЕКСТ: {context_2}\n",
    "ВОПРОС: {question_2}\n",
    "ОТВЕТ: {answer_2}\n",
    "\n",
    "3) Пример 3:\n",
    "КОНТЕКСТ: {context_3}\n",
    "ВОПРОС: {question_3}\n",
    "ОТВЕТ: {answer_3}\n",
    "\n",
    "### CONTEXT ###\n",
    "\n",
    "1) ИСТОЧНИК 1: \n",
    "- {type_of_source1}\n",
    "- {link_to_source1}\n",
    "- {batch_1}\n",
    "\n",
    "2) ИСТОЧНИК 2: \n",
    "- {type_of_source2}\n",
    "- {link_to_source2}\n",
    "- {batch_2}\n",
    "\n",
    "3) ИСТОЧНИК 3:\n",
    "- {type_of_source3}\n",
    "- {link_to_source3}\n",
    "- {batch_3}\n",
    "\n",
    "### USER ###\n",
    "\n",
    "На основании источников 1,2,3 дай исчерпывающий ответ на\n",
    "приведенный ниже вопрос. Если ты неуверен, что можешь дать ответ на основании \n",
    "источников 1,2,3 пиши «Я не могу ответить на этот вопрос.».\n",
    "- {QUESTION}\n",
    "\"\"\"\n",
    "\n",
    "sc_prompt = \"\"\"\n",
    "### CONTEXT ###\n",
    "\n",
    "1) МНЕНИЕ 1: \n",
    "- {answer1}\n",
    "\n",
    "2) МНЕНИЕ 2: \n",
    "- {answer2}\n",
    "\n",
    "3) МНЕНИЕ 3:\n",
    "- {answer3}\n",
    "\n",
    "### USER ###\n",
    "На основании мнений 1,2,3 дай итоговый ответ по принципу большинства.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "### SYSTEM ###\n",
    "Ты - дружелюбный ассистент Центрального банка Российской Федерации. \n",
    "Исчерпывающе отвечай клиентам на возникшие вопросы.\n",
    "На все вопросы не касающиеся ЦБ РФ, финансов и экономики ты обязан отвечать:\n",
    "'Давайте сменим тему.'\n",
    "\"\"\"\n",
    "\n",
    "def ask_chatbot(mode, question):\n",
    "    gc.collect()\n",
    "    try:\n",
    "        formatted_rag_prompt = \"\"\n",
    "        query = prep_query(question)\n",
    "        if mode == \"zero-shot\":\n",
    "\n",
    "            result = bm25_ensemble_with_crossenc_answer(query=query, client=client, emb_func=emb_func, bm25_n_results=15, cr_enc_n_results=3, limit_knn=100, knn_docs_window=1)\n",
    "\n",
    "            type_of_source1, type_of_source2, type_of_source3  = '', '', ''\n",
    "            link_to_source1, link_to_source2, link_to_source3 = '', '', ''\n",
    "            batch_1, batch_2, batch_3 = '', '', ''\n",
    "\n",
    "\n",
    "            for idx, batch in enumerate(start=1, iterable=result):\n",
    "                if idx==1:\n",
    "                    if batch[5]:\n",
    "                        type_of_source1 = 'ТАБЛИЦА'\n",
    "                    else:\n",
    "                        type_of_source1 = 'ТЕКСТ'\n",
    "                    link_to_source1 = batch[2]\n",
    "                    batch_1 = batch[3]\n",
    "\n",
    "                if idx==2:\n",
    "                    if batch[5]:\n",
    "                        type_of_source2 = 'ТАБЛИЦА'\n",
    "                    else:\n",
    "                        type_of_source2 = 'ТЕКСТ'\n",
    "                    link_to_source2 = batch[2]\n",
    "                    batch_2 = batch[3]\n",
    "\n",
    "                if idx==3:\n",
    "                    if batch[5]:\n",
    "                        type_of_source3 = 'ТАБЛИЦА'\n",
    "                    else:\n",
    "                        type_of_source3 = 'ТЕКСТ'\n",
    "                    link_to_source3 = batch[2]\n",
    "                    batch_3 = batch[3]\n",
    "\n",
    "\n",
    "            formatted_rag_prompt = rag_prompt.format(\n",
    "                type_of_source1=type_of_source1,\n",
    "                link_to_source1=link_to_source1,\n",
    "                batch_1=batch_1,\n",
    "                type_of_source2=type_of_source2,\n",
    "                link_to_source2=link_to_source2,\n",
    "                batch_2=batch_2,\n",
    "                type_of_source3=type_of_source3,\n",
    "                link_to_source3=link_to_source3,\n",
    "                batch_3=batch_3,\n",
    "                QUESTION=question\n",
    "            )\n",
    "\n",
    "            answer = message_llm(system_prompt, formatted_rag_prompt)\n",
    "\n",
    "            if not ('Давайте сменим тему.' in answer or 'Я не могу ответить на этот вопрос' in answer):\n",
    "                return f\"{answer}\\n\\nИсточники: \\n{link_to_source1}, \\n{link_to_source2}, \\n{link_to_source3}\"\n",
    "            else:\n",
    "                return answer\n",
    "            \n",
    "        elif mode == \"few-shot\":\n",
    "            result = bm25_ensemble_with_crossenc_answer(query=query, client=client, emb_func=emb_func, bm25_n_results=15, cr_enc_n_results=3, limit_knn=100, knn_docs_window=1)\n",
    "\n",
    "            type_of_source1, type_of_source2, type_of_source3  = '', '', ''\n",
    "            link_to_source1, link_to_source2, link_to_source3 = '', '', ''\n",
    "            batch_1, batch_2, batch_3 = '', '', ''\n",
    "\n",
    "            for idx, batch in enumerate(start=1, iterable=result):\n",
    "                if idx==1:\n",
    "                    if batch[5]:\n",
    "                        type_of_source1 = 'ТАБЛИЦА'\n",
    "                    else:\n",
    "                        type_of_source1 = 'ТЕКСТ'\n",
    "                    link_to_source1 = batch[2]\n",
    "                    batch_1 = batch[3]\n",
    "\n",
    "                if idx==2:\n",
    "                    if batch[5]:\n",
    "                        type_of_source2 = 'ТАБЛИЦА'\n",
    "                    else:\n",
    "                        type_of_source2 = 'ТЕКСТ'\n",
    "                    link_to_source2 = batch[2]\n",
    "                    batch_2 = batch[3]\n",
    "\n",
    "                if idx==3:\n",
    "                    if batch[5]:\n",
    "                        type_of_source3 = 'ТАБЛИЦА'\n",
    "                    else:\n",
    "                        type_of_source3 = 'ТЕКСТ'\n",
    "                    link_to_source3 = batch[2]\n",
    "                    batch_3 = batch[3]\n",
    "\n",
    "            few_shot_examples = bm25_fewshot_with_cross_encoder(bm25_n_results=100, cr_enc_n_results=3, df=few_shot_data, query=query)\n",
    "\n",
    "            formatted_rag_prompt = rag_prompt_fw_ex.format(\n",
    "                context_1=few_shot_examples[0]['context'],\n",
    "                question_1=few_shot_examples[0]['question'],\n",
    "                answer_1=few_shot_examples[0]['answer'],\n",
    "                context_2=few_shot_examples[1]['context'],\n",
    "                question_2=few_shot_examples[1]['question'], \n",
    "                answer_2=few_shot_examples[1]['answer'],\n",
    "                context_3=few_shot_examples[2]['context'],\n",
    "                question_3=few_shot_examples[2]['question'],  \n",
    "                answer_3=few_shot_examples[2]['answer'],  \n",
    "                type_of_source1=type_of_source1,\n",
    "                link_to_source1=link_to_source1,\n",
    "                batch_1=batch_1,\n",
    "                type_of_source2=type_of_source2,\n",
    "                link_to_source2=link_to_source2,\n",
    "                batch_2=batch_2,\n",
    "                type_of_source3=type_of_source3,\n",
    "                link_to_source3=link_to_source3,\n",
    "                batch_3=batch_3,\n",
    "                QUESTION=question\n",
    "            )\n",
    "            answer = message_llm(system_prompt, formatted_rag_prompt)\n",
    "\n",
    "            if not ('Давайте сменим тему.' in answer or 'Я не могу ответить на этот вопрос' in answer):\n",
    "                return f\"{answer}\\n\\nИсточники: \\n{link_to_source1}, \\n{link_to_source2}, \\n{link_to_source3}\"\n",
    "            else:\n",
    "                return answer\n",
    "            \n",
    "        elif mode == \"self-consistency\":\n",
    "            result = bm25_ensemble_with_crossenc_answer(query=query, client=client, emb_func=emb_func, bm25_n_results=15, cr_enc_n_results=3, limit_knn=100, knn_docs_window=1)\n",
    "\n",
    "            type_of_source1, type_of_source2, type_of_source3  = '', '', ''\n",
    "            link_to_source1, link_to_source2, link_to_source3 = '', '', ''\n",
    "            batch_1, batch_2, batch_3 = '', '', ''\n",
    "\n",
    "\n",
    "            for idx, batch in enumerate(start=1, iterable=result):\n",
    "                if idx==1:\n",
    "                    if batch[5]:\n",
    "                        type_of_source1 = 'ТАБЛИЦА'\n",
    "                    else:\n",
    "                        type_of_source1 = 'ТЕКСТ'\n",
    "                    link_to_source1 = batch[2]\n",
    "                    batch_1 = batch[3]\n",
    "\n",
    "                if idx==2:\n",
    "                    if batch[5]:\n",
    "                        type_of_source2 = 'ТАБЛИЦА'\n",
    "                    else:\n",
    "                        type_of_source2 = 'ТЕКСТ'\n",
    "                    link_to_source2 = batch[2]\n",
    "                    batch_2 = batch[3]\n",
    "\n",
    "                if idx==3:\n",
    "                    if batch[5]:\n",
    "                        type_of_source3 = 'ТАБЛИЦА'\n",
    "                    else:\n",
    "                        type_of_source3 = 'ТЕКСТ'\n",
    "                    link_to_source3 = batch[2]\n",
    "                    batch_3 = batch[3]\n",
    "\n",
    "            formatted_rag_prompt = rag_prompt.format(\n",
    "                type_of_source1=type_of_source1,\n",
    "                link_to_source1=link_to_source1,\n",
    "                batch_1=batch_1,\n",
    "                type_of_source2=type_of_source2,\n",
    "                link_to_source2=link_to_source2,\n",
    "                batch_2=batch_2,\n",
    "                type_of_source3=type_of_source3,\n",
    "                link_to_source3=link_to_source3,\n",
    "                batch_3=batch_3,\n",
    "                QUESTION=question\n",
    "            )\n",
    "\n",
    "            answer1 = message_llm(system_prompt, formatted_rag_prompt)\n",
    "            answer2 = message_llm(system_prompt, formatted_rag_prompt)\n",
    "            answer3 = message_llm(system_prompt, formatted_rag_prompt)\n",
    "\n",
    "            formatted_som_prompt = sc_prompt.format(answer1=answer1,answer2=answer2,answer3=answer3)\n",
    "            final_answer = message_llm(system_prompt, formatted_som_prompt)\n",
    "            \n",
    "            if not ('Давайте сменим тему.' in final_answer or 'Я не могу ответить на этот вопрос' in final_answer):\n",
    "                return f\"{final_answer}\\n\\nИсточники: \\n{link_to_source1}, \\n{link_to_source2}, \\n{link_to_source3}\"\n",
    "            else:\n",
    "                return final_answer\n",
    "    except Exception as e:\n",
    "        return f\"Пожалуйста, посетите FAQ страницу сайта ЦБ РФ (https://www.cbr.ru/faq/) для получения ответа на этот вопрос.\"  \n",
    "\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=ask_chatbot,\n",
    "    inputs=[\n",
    "        gr.Dropdown(choices=[\"zero-shot\", \"few-shot\", \"self-consistency\"], label=\"Выберите режим\"),\n",
    "        gr.Textbox(lines=2, placeholder=\"Введите ваш вопрос здесь...\", label=\"Вопрос\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Ответ\"),\n",
    "    title=\"Чат с виртуальным помощником - SCOOF DEVELOPERS\",\n",
    "    description=\"Выберите режим и введите ваше сообщение, чтобы получить ответ от модели:\"\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd0254e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Collecting gradio\n",
      "  Downloading gradio-4.21.0-py3-none-any.whl (17.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in ./.venv/lib/python3.10/site-packages (from gradio) (10.2.0)\n",
      "Requirement already satisfied: markupsafe~=2.0 in ./.venv/lib/python3.10/site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in ./.venv/lib/python3.10/site-packages (from gradio) (6.3.0)\n",
      "Collecting aiofiles<24.0,>=22.0\n",
      "  Using cached aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting gradio-client==0.12.0\n",
      "  Downloading gradio_client-0.12.0-py3-none-any.whl (310 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.7/310.7 KB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy~=1.0 in ./.venv/lib/python3.10/site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from gradio) (24.0)\n",
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: matplotlib~=3.0 in ./.venv/lib/python3.10/site-packages (from gradio) (3.8.3)\n",
      "Collecting semantic-version~=2.0\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: fastapi in ./.venv/lib/python3.10/site-packages (from gradio) (0.110.0)\n",
      "Collecting altair<6.0,>=4.2.0\n",
      "  Using cached altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "Requirement already satisfied: typer[all]<1.0,>=0.9 in ./.venv/lib/python3.10/site-packages (from gradio) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in ./.venv/lib/python3.10/site-packages (from gradio) (4.10.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in ./.venv/lib/python3.10/site-packages (from gradio) (0.28.0)\n",
      "Collecting httpx>=0.24.1\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in ./.venv/lib/python3.10/site-packages (from gradio) (0.21.4)\n",
      "Collecting ffmpy\n",
      "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ruff>=0.2.2\n",
      "  Downloading ruff-0.3.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tomlkit==0.12.0\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Collecting python-multipart>=0.0.9\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: orjson~=3.0 in ./.venv/lib/python3.10/site-packages (from gradio) (3.9.15)\n",
      "Requirement already satisfied: pydantic>=2.0 in ./.venv/lib/python3.10/site-packages (from gradio) (2.6.4)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in ./.venv/lib/python3.10/site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in ./.venv/lib/python3.10/site-packages (from gradio) (2.2.1)\n",
      "Requirement already satisfied: jinja2<4.0 in ./.venv/lib/python3.10/site-packages (from gradio) (3.1.3)\n",
      "Collecting websockets<12.0,>=10.0\n",
      "  Using cached websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from gradio-client==0.12.0->gradio) (2024.2.0)\n",
      "Collecting toolz\n",
      "  Using cached toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "Collecting jsonschema>=3.0\n",
      "  Using cached jsonschema-4.21.1-py3-none-any.whl (85 kB)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (3.6)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (4.3.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
      "Collecting httpcore==1.*\n",
      "  Using cached httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.49.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in ./.venv/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (2.16.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./.venv/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
      "Collecting rich<14.0.0,>=10.11.0\n",
      "  Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Collecting colorama<0.5.0,>=0.4.3\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting shellingham<2.0.0,>=1.3.0\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in ./.venv/lib/python3.10/site-packages (from fastapi->gradio) (0.36.3)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Using cached rpds_py-0.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Collecting referencing>=0.28.4\n",
      "  Using cached referencing-0.33.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./.venv/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.17.2)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio->httpx>=0.24.1->gradio) (1.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5600 sha256=810460631da1fe5d13ea682b2cd2f8285bac9e02727e791eccc3d633eedae9b6\n",
      "  Stored in directory: /home/veidlink/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
      "Successfully built ffmpy\n",
      "Installing collected packages: pydub, ffmpy, websockets, toolz, tomlkit, shellingham, semantic-version, ruff, rpds-py, python-multipart, mdurl, httpcore, colorama, aiofiles, referencing, markdown-it-py, httpx, rich, jsonschema-specifications, gradio-client, jsonschema, altair, gradio\n",
      "  Attempting uninstall: websockets\n",
      "    Found existing installation: websockets 12.0\n",
      "    Uninstalling websockets-12.0:\n",
      "      Successfully uninstalled websockets-12.0\n",
      "Successfully installed aiofiles-23.2.1 altair-5.2.0 colorama-0.4.6 ffmpy-0.3.2 gradio-4.21.0 gradio-client-0.12.0 httpcore-1.0.4 httpx-0.27.0 jsonschema-4.21.1 jsonschema-specifications-2023.12.1 markdown-it-py-3.0.0 mdurl-0.1.2 pydub-0.25.1 python-multipart-0.0.9 referencing-0.33.0 rich-13.7.1 rpds-py-0.18.0 ruff-0.3.2 semantic-version-2.10.0 shellingham-1.5.4 tomlkit-0.12.0 toolz-0.12.1 websockets-11.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
