{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdb6ffe6",
   "metadata": {},
   "source": [
    "**Интерфейс для удобного общения с виртуальным помощником:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16abcbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/gblssroman/anaconda3/lib/python3.11/site-packages/gradio/queueing.py\", line 501, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gblssroman/anaconda3/lib/python3.11/site-packages/gradio/route_utils.py\", line 253, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gblssroman/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1695, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gblssroman/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1235, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gblssroman/anaconda3/lib/python3.11/site-packages/anyio/to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gblssroman/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/gblssroman/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gblssroman/anaconda3/lib/python3.11/site-packages/gradio/utils.py\", line 692, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_8786/293429438.py\", line 226, in ask_chatbot\n",
      "    formatted_examples = '\\n\\n'.join([f\"Пример {i+1}:\\nКОНТЕКСТ: {ex['context']}\\nВОПРОС: {ex['question']}\\nОТВЕТ: {ex['answer']}\" for i, ex in enumerate(few_shot_examples)])\n",
      "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_8786/293429438.py\", line 226, in <listcomp>\n",
      "    formatted_examples = '\\n\\n'.join([f\"Пример {i+1}:\\nКОНТЕКСТ: {ex['context']}\\nВОПРОС: {ex['question']}\\nОТВЕТ: {ex['answer']}\" for i, ex in enumerate(few_shot_examples)])\n",
      "                                                                  ~~^^^^^^^^^^^\n",
      "KeyError: 'context'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import clickhouse_connect\n",
    "from embedding import E5LargeEmbeddingFunction\n",
    "from similarity import bm25_ensemble_with_crossenc_answer, prep_query, bm25_fewshot_with_cross_encoder\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "\n",
    "client = clickhouse_connect.get_client(host='y1jzidyt9q.us-east-2.aws.clickhouse.cloud', port=8443, username='default', password='_lQ_JWXYQD3ym')\n",
    "emb_func = E5LargeEmbeddingFunction()\n",
    "\n",
    "def message_llm(system_prompt, user_input, temperature=0.6):\n",
    "    '''Function to message (test) our LLM'''\n",
    "\n",
    "    try:\n",
    "        url = \"https://live-relaxed-oryx.ngrok-free.app/v1/chat/completions\"\n",
    "\n",
    "        data = {\n",
    "          \"messages\": [\n",
    "            { \"role\": \"system\", \"content\": system_prompt},\n",
    "            { \"role\": \"user\", \"content\": user_input}\n",
    "          ],\n",
    "          \"temperature\": 0.6,\n",
    "          # \"max_tokens\": -1,\n",
    "          # \"stream\": False\n",
    "        }\n",
    "\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return eval(response.text.replace(\"\\n  \", \"\"))['choices'][0]['message']['content']\n",
    "        else:\n",
    "            print(\"Ошибка запроса:\\n\", response.status_code, response.text)\n",
    "            return \"\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        \n",
    "        print(f\"Error {e}!\")\n",
    "        \n",
    "rag_prompt = \"\"\"\n",
    "### CONTEXT ###\n",
    "1) ИСТОЧНИК 1: \n",
    "- {type_of_source1}\n",
    "- {link_to_source1}\n",
    "- {batch_1}\n",
    "\n",
    "2) ИСТОЧНИК 2: \n",
    "- {type_of_source2}\n",
    "- {link_to_source2}\n",
    "- {batch_2}\n",
    "\n",
    "3) ИСТОЧНИК 3:\n",
    "- {type_of_source3}\n",
    "- {link_to_source3}\n",
    "- {batch_3}\n",
    "\n",
    "### INSTRUCTION ###\n",
    "На основании приведенных данных категории «ТЕКСТ» дай исчерпывающий ответ на\n",
    "приведенный ниже вопрос:\n",
    "- {QUESTION}\n",
    "\"\"\"\n",
    "\n",
    "few_shot_data = pd.read_csv('qa_generation/finetune_df_final.csv', index_col=0)\n",
    "few_shot_data\n",
    "\n",
    "rag_prompt_fw_ex = \"\"\"\n",
    "### EXAMPLES ###\n",
    "\n",
    "1) Пример 1:\n",
    "КОНТЕКСТ: {context_1}\n",
    "ВОПРОС: {question_1}\n",
    "ОТВЕТ: {answer_1}\n",
    "\n",
    "2) Пример 2:\n",
    "КОНТЕКСТ: {context_2}\n",
    "ВОПРОС: {question_2}\n",
    "ОТВЕТ: {answer_2}\n",
    "\n",
    "3) Пример 3:\n",
    "КОНТЕКСТ: {context_3}\n",
    "ВОПРОС: {question_3}\n",
    "ОТВЕТ: {answer_3}\n",
    "\n",
    "### CONTEXT ###\n",
    "\n",
    "1) ИСТОЧНИК 1: \n",
    "- {type_of_source1}\n",
    "- {link_to_source1}\n",
    "- {batch_1}\n",
    "\n",
    "2) ИСТОЧНИК 2: \n",
    "- {type_of_source2}\n",
    "- {link_to_source2}\n",
    "- {batch_2}\n",
    "\n",
    "3) ИСТОЧНИК 3:\n",
    "- {type_of_source3}\n",
    "- {link_to_source3}\n",
    "- {batch_3}\n",
    "\n",
    "### INSTRUCTION ###\n",
    "\n",
    "На основании приведенных данных категории «ТЕКСТ» дай исчерпывающий ответ на\n",
    "приведенный ниже вопрос:\n",
    "- {QUESTION}\n",
    "\"\"\"\n",
    "\n",
    "sc_prompt = \"\"\"\n",
    "### CONTEXT ###\n",
    "\n",
    "1) МНЕНИЕ 1: \n",
    "- {answer1}\n",
    "\n",
    "2) МНЕНИЕ 2: \n",
    "- {answer2}\n",
    "\n",
    "3) МНЕНИЕ 3:\n",
    "- {answer3}\n",
    "\n",
    "### INSTRUCTION ###\n",
    "На основании приведенного контекста, дай итоговый ответ клиенту на его вопрос по принципу большинства.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "### ROLE ###\n",
    "Ты - дружелюбный ассистент Центрального банка Российской Федерации. \n",
    "Исчерпывающе отвечай клиентам на возникшие вопросы. Если из контекста \n",
    "невозможно дать ответ на вопрос, пиши «Я не могу ответить на этот вопрос».\"\"\"\n",
    "\n",
    "def ask_chatbot(mode, question):\n",
    "    gc.collect()\n",
    "    try:\n",
    "        formatted_rag_prompt = \"\"\n",
    "        query = prep_query(question)\n",
    "        if mode == \"zero-shot\":\n",
    "\n",
    "            result = bm25_ensemble_with_crossenc_answer(query=query, client=client, emb_func=emb_func, bm25_n_results=15, cr_enc_n_results=3, limit_knn=100, knn_docs_window=1)\n",
    "\n",
    "            type_of_source1, type_of_source2, type_of_source3  = '', '', ''\n",
    "            link_to_source1, link_to_source2, link_to_source3 = '', '', ''\n",
    "            batch_1, batch_2, batch_3 = '', '', ''\n",
    "\n",
    "\n",
    "            for idx, batch in enumerate(start=1, iterable=result):\n",
    "                if idx==1:\n",
    "                    if batch[5]:\n",
    "                        type_of_source1 = 'ТАБЛИЦА'\n",
    "                    else:\n",
    "                        type_of_source1 = 'ТЕКСТ'\n",
    "                    link_to_source1 = batch[2]\n",
    "                    batch_1 = batch[3]\n",
    "\n",
    "                if idx==2:\n",
    "                    if batch[5]:\n",
    "                        type_of_source2 = 'ТАБЛИЦА'\n",
    "                    else:\n",
    "                        type_of_source2 = 'ТЕКСТ'\n",
    "                    link_to_source2 = batch[2]\n",
    "                    batch_2 = batch[3]\n",
    "\n",
    "                if idx==3:\n",
    "                    if batch[5]:\n",
    "                        type_of_source3 = 'ТАБЛИЦА'\n",
    "                    else:\n",
    "                        type_of_source3 = 'ТЕКСТ'\n",
    "                    link_to_source3 = batch[2]\n",
    "                    batch_3 = batch[3]\n",
    "\n",
    "\n",
    "            formatted_rag_prompt = rag_prompt.format(\n",
    "                type_of_source1=type_of_source1,\n",
    "                link_to_source1=link_to_source1,\n",
    "                batch_1=batch_1,\n",
    "                type_of_source2=type_of_source2,\n",
    "                link_to_source2=link_to_source2,\n",
    "                batch_2=batch_2,\n",
    "                type_of_source3=type_of_source3,\n",
    "                link_to_source3=link_to_source3,\n",
    "                batch_3=batch_3,\n",
    "                QUESTION=question\n",
    "            )\n",
    "\n",
    "            answer = message_llm(system_prompt, formatted_rag_prompt)\n",
    "\n",
    "            return f\"{answer}\\nИсточники: \\n{link_to_source1}, \\n{link_to_source2}, \\n{link_to_source3}\"\n",
    "\n",
    "        elif mode == \"few-shot\":\n",
    "            result = bm25_ensemble_with_crossenc_answer(query=query, client=client, emb_func=emb_func, bm25_n_results=15, cr_enc_n_results=3, limit_knn=100, knn_docs_window=1)\n",
    "\n",
    "            type_of_source1, type_of_source2, type_of_source3  = '', '', ''\n",
    "            link_to_source1, link_to_source2, link_to_source3 = '', '', ''\n",
    "            batch_1, batch_2, batch_3 = '', '', ''\n",
    "\n",
    "            for idx, batch in enumerate(start=1, iterable=result):\n",
    "                if idx==1:\n",
    "                    if batch[5]:\n",
    "                        type_of_source1 = 'ТАБЛИЦА'\n",
    "                    else:\n",
    "                        type_of_source1 = 'ТЕКСТ'\n",
    "                    link_to_source1 = batch[2]\n",
    "                    batch_1 = batch[3]\n",
    "\n",
    "                if idx==2:\n",
    "                    if batch[5]:\n",
    "                        type_of_source2 = 'ТАБЛИЦА'\n",
    "                    else:\n",
    "                        type_of_source2 = 'ТЕКСТ'\n",
    "                    link_to_source2 = batch[2]\n",
    "                    batch_2 = batch[3]\n",
    "\n",
    "                if idx==3:\n",
    "                    if batch[5]:\n",
    "                        type_of_source3 = 'ТАБЛИЦА'\n",
    "                    else:\n",
    "                        type_of_source3 = 'ТЕКСТ'\n",
    "                    link_to_source3 = batch[2]\n",
    "                    batch_3 = batch[3]\n",
    "\n",
    "            few_shot_examples = bm25_fewshot_with_cross_encoder(bm25_n_results=100, cr_enc_n_results=3, df=few_shot_data, query=query)\n",
    "            formatted_examples = '\\n\\n'.join([f\"Пример {i+1}:\\nКОНТЕКСТ: {ex['context']}\\nВОПРОС: {ex['question']}\\nОТВЕТ: {ex['answer']}\" for i, ex in enumerate(few_shot_examples)])\n",
    "\n",
    "            formatted_rag_prompt = rag_prompt_fw_ex.format(\n",
    "                context_1=few_shot_examples[0]['context'],\n",
    "                question_1=few_shot_examples[0]['question'],\n",
    "                answer_1=few_shot_examples[0]['answer'],\n",
    "                context_2=few_shot_examples[1]['context'],\n",
    "                question_2=few_shot_examples[1]['question'], \n",
    "                answer_2=few_shot_examples[1]['answer'],\n",
    "                context_3=few_shot_examples[2]['context'],\n",
    "                question_3=few_shot_examples[2]['question'],  \n",
    "                answer_3=few_shot_examples[2]['answer'],  \n",
    "                type_of_source1=type_of_source1,\n",
    "                link_to_source1=link_to_source1,\n",
    "                batch_1=batch_1,\n",
    "                type_of_source2=type_of_source2,\n",
    "                link_to_source2=link_to_source2,\n",
    "                batch_2=batch_2,\n",
    "                type_of_source3=type_of_source3,\n",
    "                link_to_source3=link_to_source3,\n",
    "                batch_3=batch_3,\n",
    "                QUESTION=question\n",
    "            )\n",
    "            answer = message_llm(system_prompt, formatted_rag_prompt)\n",
    "\n",
    "            return f\"{answer}\\nИсточники: \\n{link_to_source1}, \\n{link_to_source2}, \\n{link_to_source3}\"\n",
    "\n",
    "        elif mode == \"self-consistency\":\n",
    "            result = bm25_ensemble_with_crossenc_answer(query=query, client=client, emb_func=emb_func, bm25_n_results=15, cr_enc_n_results=3, limit_knn=100, knn_docs_window=1)\n",
    "\n",
    "            type_of_source1, type_of_source2, type_of_source3  = '', '', ''\n",
    "            link_to_source1, link_to_source2, link_to_source3 = '', '', ''\n",
    "            batch_1, batch_2, batch_3 = '', '', ''\n",
    "\n",
    "\n",
    "            for idx, batch in enumerate(start=1, iterable=result):\n",
    "                if idx==1:\n",
    "                    if batch[5]:\n",
    "                        type_of_source1 = 'ТАБЛИЦА'\n",
    "                    else:\n",
    "                        type_of_source1 = 'ТЕКСТ'\n",
    "                    link_to_source1 = batch[2]\n",
    "                    batch_1 = batch[3]\n",
    "\n",
    "                if idx==2:\n",
    "                    if batch[5]:\n",
    "                        type_of_source2 = 'ТАБЛИЦА'\n",
    "                    else:\n",
    "                        type_of_source2 = 'ТЕКСТ'\n",
    "                    link_to_source2 = batch[2]\n",
    "                    batch_2 = batch[3]\n",
    "\n",
    "                if idx==3:\n",
    "                    if batch[5]:\n",
    "                        type_of_source3 = 'ТАБЛИЦА'\n",
    "                    else:\n",
    "                        type_of_source3 = 'ТЕКСТ'\n",
    "                    link_to_source3 = batch[2]\n",
    "                    batch_3 = batch[3]\n",
    "\n",
    "            formatted_rag_prompt = rag_prompt.format(\n",
    "                type_of_source1=type_of_source1,\n",
    "                link_to_source1=link_to_source1,\n",
    "                batch_1=batch_1,\n",
    "                type_of_source2=type_of_source2,\n",
    "                link_to_source2=link_to_source2,\n",
    "                batch_2=batch_2,\n",
    "                type_of_source3=type_of_source3,\n",
    "                link_to_source3=link_to_source3,\n",
    "                batch_3=batch_3,\n",
    "                QUESTION=question\n",
    "            )\n",
    "\n",
    "            answer1 = message_llm(system_prompt, formatted_rag_prompt)\n",
    "            answer2 = message_llm(system_prompt, formatted_rag_prompt)\n",
    "            answer3 = message_llm(system_prompt, formatted_rag_prompt)\n",
    "\n",
    "            formatted_som_prompt = sc_prompt.format(answer1=answer1,answer2=answer2,answer3=answer3)\n",
    "            final_answer = message_llm(system_prompt, formatted_som_prompt)\n",
    "\n",
    "            return f\"{final_answer}\\nИсточники: \\n{link_to_source1}, \\n{link_to_source2}, \\n{link_to_source3}\"\n",
    "    except Exception as e:\n",
    "        return f\"Пожалуйста, посетите FAQ страницу сайта ЦБ РФ (https://www.cbr.ru/faq/) для получения ответа на этот вопрос.\"  \n",
    "\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=ask_chatbot,\n",
    "    inputs=[\n",
    "        gr.Dropdown(choices=[\"zero-shot\", \"few-shot\", \"self-consistency\"], label=\"Выберите режим\"),\n",
    "        gr.Textbox(lines=2, placeholder=\"Введите ваш вопрос здесь...\", label=\"Вопрос\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Ответ\"),\n",
    "    title=\"Чат с виртуальным помощником - SCOOF DEVELOPERS\",\n",
    "    description=\"Выберите режим и введите ваше сообщение, чтобы получить ответ от модели:\"\n",
    ")\n",
    "\n",
    "# Запуск интерфейса\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0254e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
